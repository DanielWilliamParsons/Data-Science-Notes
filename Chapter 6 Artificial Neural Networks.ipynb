{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Networks\n",
    "\n",
    "Previously, we looked at the case where $h_{\\theta}(x) = \\theta^{T}x$, possible in linear and logistic regression. This model is linear in $\\theta$. ANNs are models that are non-linear in both the parameters $\\theta$ and the inputs $x$.\n",
    "\n",
    "Suppose $\\{(x^{(i)}, y^{(i)})\\}_{i=1}^{n}$ are the training examples and $y^{(i)} \\in \\mathbb{R}$ and $h_{\\theta}(x) \\in \\mathbb{R}$.\n",
    "\n",
    "If we define the least squares cost function for the $i$-th example as $J^{(i)}(\\theta) = \\frac{1}{2}(h_{\\theta}(x^{(i)}) - y^{(i)})^{2}$, then we can also define the mean-square cost function for the dataset as:\n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{n} \\sum_{i=1}^{n}J^{(i)}(\\theta) $$\n",
    "\n",
    "which looks just the same as for linear regression except there is now a constant $\\frac{1}{n}$ in front."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient descent\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a neural network?\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
